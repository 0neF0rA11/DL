{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Задание 1. Загрузите текст из произведений Ницше ('nietzsche.txt', origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')."
      ],
      "metadata": {
        "id": "K33xMGq6G3oC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0owrsuMFvsR",
        "outputId": "2667ff4d-f2c7-4ffc-9334-72eef82516e2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "from urllib.request import urlretrieve\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "URL = r\"https://s3.amazonaws.com/text-datasets/nietzsche.txt\"\n",
        "asset_path = os.path.join(os.getcwd(), f\"nietzsche.txt\")\n",
        "\n",
        "if not os.path.exists(asset_path):\n",
        "    print(f\"Downloading and extracting assests....\", end=\"\")\n",
        "    urlretrieve(URL, asset_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBhF7nyiHlZw",
        "outputId": "da679b58-a0b4-44e4-fd16-b0d510110cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and extracting assests...."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"nietzsche.txt\", 'r', encoding='UTF-8') as file:\n",
        "  text = file.read()\n",
        "  print(text[:100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whzGFEEBJFai",
        "outputId": "960d0215-ae5b-40ee-9db7-3c036a56b872"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREFACE\n",
            "\n",
            "\n",
            "SUPPOSING that Truth is a woman--what then? Is there not ground\n",
            "for suspecting that all ph\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(text: str, deep_preproc: int = 2) -> str:\n",
        "  if deep_preproc == 1:\n",
        "    text = re.sub(r\"\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\t\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "  text = re.sub(r\"[^A-Za-zА-Яа-я0-9]\", \" \", text)\n",
        "  text = re.sub(r\"\\s+\", \" \", text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "VWo2aRAhKJu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Длина всего текста до обработки\n",
        "print(f\"Text length without preprocessing: {len(text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQuOT2wMPZ63",
        "outputId": "f43f1f47-eca0-4b84-c3c8-01924a8e857d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text length without preprocessing: 600893\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modern_text = text_preprocessing(text, 1)\n",
        "modern_text[:100]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IaK_xWxELQl5",
        "outputId": "35f0e5a6-15b6-4e26-92b5-96e1147d82b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'PREFACE SUPPOSING that Truth is a woman--what then? Is there not ground for suspecting that all phil'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(modern_text)\n",
        "treated_sentences = [text_preprocessing(sentence).strip() for sentence in sentences if sentence.count(\" \") >= 1]\n",
        "treated_sentences[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hM7C0cwEOD00",
        "outputId": "569a5f05-a051-4f6d-c583-c63308136172"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PREFACE SUPPOSING that Truth is a woman what then',\n",
              " 'Is there not ground for suspecting that all philosophers in so far as they have been dogmatists have failed to understand women that the terrible seriousness and clumsy importunity with which they have usually paid their addresses to Truth have been unskilled and unseemly methods for winning a woman',\n",
              " 'Certainly she has never allowed herself to be won and at present every kind of dogma stands with sad and discouraged mien IF indeed it stands at all',\n",
              " 'For there are scoffers who maintain that it has fallen that all dogma lies on the ground nay more that it is at its last gasp',\n",
              " 'But to speak seriously there are good grounds for hoping that all dogmatizing in philosophy whatever solemn whatever conclusive and decided airs it has assumed may have been only a noble puerilism and tyronism and probably the time is at hand when it will be once and again understood WHAT has actually sufficed for the basis of such imposing and absolute philosophical edifices as the dogmatists have hitherto reared perhaps some popular superstition of immemorial time such as the soul superstition which in the form of subject and ego superstition has not yet ceased doing mischief perhaps some play upon words a deception on the part of grammar or an audacious generalization of very restricted very personal very human all too human facts']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Количество предложений\n",
        "print(f\"Number of sentences: {len(treated_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxnRSsSrUZuy",
        "outputId": "94470e6a-b72a-4ae6-b514-fded9dcf5d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of sentences: 2847\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Количество слов\n",
        "words = ' '.join(treated_sentences).split(\" \")\n",
        "unique_words = set(words)\n",
        "print(f\"Count of (unique words)/(words): {len(unique_words)}/{len(words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wCYAcU9EW_Q9",
        "outputId": "be997dc8-c59c-475c-8e28-b0c61d13a516"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Count of (unique words)/(words): 11363/101251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Количество всех символов после обработки:\n",
        "new_text = '.'.join(treated_sentences)\n",
        "print(f\"Text length past processing: {len(new_text)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b-Iz7yUWiQQ",
        "outputId": "919d202f-e725-43d8-d613-c889f10a31ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text length past processing: 579860\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 2. Сократите текст наполовину избыточными последовательностями символов maxlen"
      ],
      "metadata": {
        "id": "owmXhmj9YQmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cut_text(text: str, maxlen: int) -> str:\n",
        "  if len(text) > maxlen:\n",
        "    text += \" \"\n",
        "    index_cut = text.index(\" \", maxlen)\n",
        "    shortened_text = text[:index_cut]\n",
        "    return shortened_text\n",
        "  return text"
      ],
      "metadata": {
        "id": "VbzXbYLlYEer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cut_sentences = [cut_text(sentence, maxlen=len(sentence) // 2) for sentence in sentences if sentence.count(\" \") >= 1 ]\n",
        "cut_sentences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML3mbS3yoHo1",
        "outputId": "26c1ac2d-cb1e-463e-cc4e-30100cb7649b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['PREFACE SUPPOSING that Truth',\n",
              " 'Is there not ground for suspecting that all philosophers, in so far as they have been dogmatists, have failed to understand women--that the terrible seriousness',\n",
              " 'Certainly she has never allowed herself to be won; and at present every kind',\n",
              " 'For there are scoffers who maintain that it has fallen, that all',\n",
              " 'But to speak seriously, there are good grounds for hoping that all dogmatizing in philosophy, whatever solemn, whatever conclusive and decided airs it has assumed, may have been only a noble puerilism and tyronism; and probably the time is at hand when it will be once and again understood WHAT has actually sufficed for the basis of such imposing and absolute philosophical edifices',\n",
              " 'The philosophy of the dogmatists, it is to be hoped, was only a promise for thousands of years afterwards, as was astrology in still earlier times, in the service of which probably more labour,',\n",
              " 'It seems that in order to inscribe themselves upon the heart of humanity with everlasting claims, all great things have first to wander about the earth as enormous',\n",
              " 'Let us not be ungrateful to it, although it must certainly be confessed that the worst, the most tiresome, and the most dangerous',\n",
              " 'But now when it has been surmounted, when Europe, rid of this nightmare, can again draw breath freely and at least enjoy a healthier--sleep,',\n",
              " 'It amounted to the very inversion of truth, and the denial of the PERSPECTIVE--the fundamental condition--of life, to speak of Spirit and the']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 3. Создайте модель LSTM для генерации текста"
      ],
      "metadata": {
        "id": "2IB15xMgyQnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "UwH5D4r1xcAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"CUDA доступна\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"CUDA недоступна, используется CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hu0gjRd01LRb",
        "outputId": "3ff01240-82c7-4532-f2f0-191abec40104"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA доступна\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_to_seq(text_sample):\n",
        "    char_counts = Counter(text_sample)\n",
        "    char_counts = sorted(char_counts.items(), key = lambda x: x[1], reverse=True)\n",
        "\n",
        "    sorted_chars = [char for char, _ in char_counts]\n",
        "    char_to_idx = {char: index for index, char in enumerate(sorted_chars)}\n",
        "    idx_to_char = {v: k for k, v in char_to_idx.items()}\n",
        "    sequence = np.array([char_to_idx[char] for char in text_sample])\n",
        "\n",
        "    return sequence, char_to_idx, idx_to_char"
      ],
      "metadata": {
        "id": "5aiVb9CDySX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequence, char_to_idx, idx_to_char = text_to_seq('. '.join(cut_sentences))"
      ],
      "metadata": {
        "id": "FMVQAdz9yiSA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Генерация train и target(сдвик на одну букву)\n",
        "SEQ_LEN = 256\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "def get_batch(sequence):\n",
        "    trains = []\n",
        "    targets = []\n",
        "    for _ in range(BATCH_SIZE):\n",
        "        batch_start = np.random.randint(0, len(sequence) - SEQ_LEN)\n",
        "        chunk = sequence[batch_start: batch_start + SEQ_LEN]\n",
        "        train = torch.LongTensor(chunk[:-1]).view(-1, 1)\n",
        "        target = torch.LongTensor(chunk[1:]).view(-1, 1)\n",
        "        trains.append(train)\n",
        "        targets.append(target)\n",
        "    return torch.stack(trains, dim=0), torch.stack(targets, dim=0)"
      ],
      "metadata": {
        "id": "Dvah_u3Rz7jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для генерации текста после обучения сетки\n",
        "def evaluate(model, char_to_idx, idx_to_char, start_text=' ', prediction_len=200, temp=0.3):\n",
        "    hidden = model.init_hidden()\n",
        "    idx_input = [char_to_idx[char] for char in start_text]\n",
        "    train = torch.LongTensor(idx_input).view(-1, 1, 1).to(device)\n",
        "    predicted_text = start_text\n",
        "\n",
        "    _, hidden = model(train, hidden)\n",
        "\n",
        "    inp = train[-1].view(-1, 1, 1)\n",
        "\n",
        "    for i in range(prediction_len):\n",
        "        output, hidden = model(inp.to(device), hidden)\n",
        "        output_logits = output.cpu().data.view(-1)\n",
        "        p_next = F.softmax(output_logits / temp, dim=-1).detach().cpu().data.numpy()\n",
        "        top_index = np.random.choice(len(char_to_idx), p=p_next)\n",
        "        inp = torch.LongTensor([top_index]).view(-1, 1, 1).to(device)\n",
        "        predicted_char = idx_to_char[top_index]\n",
        "        predicted_text += predicted_char\n",
        "\n",
        "    return predicted_text"
      ],
      "metadata": {
        "id": "KFTSnGhE0atP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TextRNN(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_size, embedding_size, n_layers=1):\n",
        "        super(TextRNN, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        self.encoder = nn.Embedding(self.input_size, self.embedding_size)\n",
        "        self.lstm = nn.LSTM(self.embedding_size, self.hidden_size, self.n_layers)\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.input_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        x = self.encoder(x).squeeze(2)\n",
        "        out, (ht1, ct1) = self.lstm(x, hidden)\n",
        "        out = self.dropout(out)\n",
        "        x = self.fc(out)\n",
        "        return x, (ht1, ct1)\n",
        "\n",
        "    def init_hidden(self, batch_size=1):\n",
        "        return (torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device),\n",
        "               torch.zeros(self.n_layers, batch_size, self.hidden_size, requires_grad=True).to(device))"
      ],
      "metadata": {
        "id": "S9hN8bG71WhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = TextRNN(input_size=len(idx_to_char), hidden_size=128, embedding_size=128, n_layers=2)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    patience=5,\n",
        "    verbose=True,\n",
        "    factor=0.5\n",
        ")\n",
        "\n",
        "n_epochs = 5000\n",
        "loss_avg = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train, target = get_batch(sequence)\n",
        "    train = train.permute(1, 0, 2).to(device)\n",
        "    target = target.permute(1, 0, 2).to(device)\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "    output, hidden = model(train, hidden)\n",
        "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_avg.append(loss.item())\n",
        "    if len(loss_avg) >= 50 and epoch % 500 == 0:\n",
        "        mean_loss = np.mean(loss_avg)\n",
        "        print(f'Loss: {mean_loss}')\n",
        "        scheduler.step(mean_loss)\n",
        "        loss_avg = []\n",
        "        model.eval()\n",
        "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
        "        print(predicted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvvhzZ1Q1yWt",
        "outputId": "f9b3e2a5-ab42-44ca-843f-f930ee3f3006"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.9653057395341154\n",
            " a precise to the sense of its and the according of the present and is a prificient the can the more are the self--the present of the propent the here the according thought as the self who precisely an\n",
            "Loss: 1.515013620376587\n",
            " and and result of the morality of a souls and more the proposite in the action of a sense and his precisely and strong the world in the sense of the sense and and and puritation of the action of the a\n",
            "Loss: 1.4209165790081024\n",
            " has the same present the present the tradition of the sense of the old constraint that the service and present serious the problem of the soul the constitute and personal intercourse of the fact of th\n",
            "Loss: 1.3768407685756683\n",
            " the consequences and sense of the state that is a conscience of the most present the subject of the present the most possible of the \"the other desire the contemples of the contrary and every men of t\n",
            "Loss: 1.3470187165737153\n",
            " and sense of the problem of the desire the more the same and sense of the existence of the most or such a man as the more an action of the sense and seems to the same and has the morality and seems th\n",
            "Loss: 1.3255324313640595\n",
            " of the sense of the heart of the old and the principally the extent of the saint in the fact the instance, and and the probably the state of the strong the saint is a senses of the problem of the inst\n",
            "Loss: 1.3144642210006714\n",
            " of the proposition, the proportion of the most senses, and the soul of the most constraint to the interpretation of the strive are a world of the proportions of the opinion of the same will only the i\n",
            "Loss: 1.2981562705039977\n",
            " and all the readily and intellectual in the strength and the philosophers and the constitution of the tragic and the heart, the fear of many an advance that the spirit of the contrary, and has to be s\n",
            "Loss: 1.2898732612133026\n",
            " are be sure the consequently and in the same time to the last and and and the assertion of the world and the superstition of the external tender the sense of the state of the distrust and scientific m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(evaluate(\n",
        "    model,\n",
        "    char_to_idx,\n",
        "    idx_to_char,\n",
        "    start_text='Hello',\n",
        "    temp=0.3,\n",
        "    prediction_len=100,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tc9B_V5X17Bg",
        "outputId": "b32998b4-2472-44b6-c0b8-cd78ba1d3bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello in the desire to the problem of the sense of the sense of the highest and a man of the self-experien\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание 4. Создайте самостоятельно генерацию текста для РУССКОЯЗЫЧНОГО НАБОРА глав Wikibooks.\n",
        "Полный текст Wikibooks содержит более 270000 глав на 12 языках https://www.kaggle.com/datasets/dhruvildave/wikibooks-dataset/data\n"
      ],
      "metadata": {
        "id": "eKeK0LAE7sH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"dostoevsky.txt\", 'r', encoding='UTF-8') as file:\n",
        "  text = file.read()"
      ],
      "metadata": {
        "id": "T6LLprDuzZrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "modern_text = text_preprocessing(text, 1)\n",
        "\n",
        "sentences = sent_tokenize(modern_text)\n",
        "\n",
        "sequence, char_to_idx, idx_to_char = text_to_seq('. '.join(sentences))"
      ],
      "metadata": {
        "id": "Nx-eyji4-XXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = TextRNN(input_size=len(idx_to_char), hidden_size=128, embedding_size=128, n_layers=2)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    patience=5,\n",
        "    verbose=True,\n",
        "    factor=0.5\n",
        ")\n",
        "\n",
        "n_epochs = 50000\n",
        "loss_avg = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train, target = get_batch(sequence)\n",
        "    train = train.permute(1, 0, 2).to(device)\n",
        "    target = target.permute(1, 0, 2).to(device)\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "    output, hidden = model(train, hidden)\n",
        "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_avg.append(loss.item())\n",
        "    if len(loss_avg) >= 50 and epoch % 5000 == 0:\n",
        "        mean_loss = np.mean(loss_avg)\n",
        "        print(f'Loss: {mean_loss}')\n",
        "        scheduler.step(mean_loss)\n",
        "        loss_avg = []\n",
        "        model.eval()\n",
        "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
        "        print(predicted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXG9jT5q-qxv",
        "outputId": "408af848-2f28-4869-dbf3-737a31abf380"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.9209595162542885\n",
            " наше совсем не присороткой столь, что все раз все совсем не уж сказать на потому что он все все собой в состоянно просто на притом и все это не потому сказал и случает от положительно старика не вывор\n",
            "Loss: 1.6677124805688859\n",
            " от себя совсем после просил с самому и в ответил он от вам в нем гораздо не могу, что и все это сказала на вас только в собой и в просто все не в совершенно совершенно просто и не по не обо мной в сов\n",
            "Loss: 1.6304359251737595\n",
            " вас сердце своих дело старик, и не вы только столько подставила и пристально и все проститься с своего объяснить с ней пример и обратился в которой с себя с ним старик и столько было совершенно себя и\n",
            "Loss: 1.6124306474685668\n",
            " под весь в себя с нем стороны, а вот все сами совершенно совсем даже по своей никогда не странности его не способность и не подумал и одно словом он тогда стало быть, но вы не совершенно себя в столе \n",
            "Loss: 1.603707464814186\n",
            " в совершенно подле с ней происходить в себя обращение и совсем не запрем с себе и вот только в себе и совсем не знаю, что вы не мог просто прочел на стол, что это стало быть, с нее по остановиться в э\n",
            "Loss: 1.597076786494255\n",
            " ней в этом не то и обратился и на столь совершенно в последней поступить в своими стороны, и вот вы все это совсем стали только совсем не было с себя стоял он от себя не последней самому только в себе\n",
            "Loss: 1.5924936980724336\n",
            " ней он в самом деле с него не сказал с себя, и как будто от себя в себя показать с себя слышали своем было после нечего на которого он предложение получить она подвигали с нем в нем пристально с себя \n",
            "Loss: 1.5884864623069763\n",
            " подавать и все после последней комнате, как бы стоял в самом деле в себе с последнее поступить и принесет своих на последней возможно в каком должны в себя и не припоминает на которое совсем не послуш\n",
            "Loss: 1.5850391817569733\n",
            " ней и все тотчас же на все случай самый стороны и после не припоминать и пристально в самом деле не припоминать в какой-то больше на меня и смотрела в нем вот то же с вами посмотрел на него слова пове\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(evaluate(\n",
        "    model,\n",
        "    char_to_idx,\n",
        "    idx_to_char,\n",
        "    start_text = 'А по темным улицам гуляет дождь',\n",
        "    temp=0.3,\n",
        "    prediction_len=100,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbL1bnxmA9sB",
        "outputId": "15de0f07-7742-4b58-8403-29566ee42683"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "А по темным улицам гуляет дождь, и он стало быть, не в том, что уже совсем ничего не вы смеялись с своей последнее столько стороны \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Удалим стопслова"
      ],
      "metadata": {
        "id": "qem0DxPHCBtA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "words = text.split()\n",
        "filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "filtered_text = ' '.join(filtered_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Isj1rlzHCDVz",
        "outputId": "cc2eef71-78b6-4931-e3c4-1c42e698cf95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "modern_text = text_preprocessing(filtered_text, 1)\n",
        "\n",
        "sentences = sent_tokenize(modern_text)\n",
        "\n",
        "sequence, char_to_idx, idx_to_char = text_to_seq('. '.join(sentences))"
      ],
      "metadata": {
        "id": "R57p7aqCCSuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "model = TextRNN(input_size=len(idx_to_char), hidden_size=128, embedding_size=128, n_layers=2)\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2, amsgrad=True)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer,\n",
        "    patience=5,\n",
        "    verbose=True,\n",
        "    factor=0.5\n",
        ")\n",
        "\n",
        "n_epochs = 50000\n",
        "loss_avg = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    train, target = get_batch(sequence)\n",
        "    train = train.permute(1, 0, 2).to(device)\n",
        "    target = target.permute(1, 0, 2).to(device)\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "\n",
        "    output, hidden = model(train, hidden)\n",
        "    loss = criterion(output.permute(1, 2, 0), target.squeeze(-1).permute(1, 0))\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    loss_avg.append(loss.item())\n",
        "    if len(loss_avg) >= 50 and epoch % 5000 == 0:\n",
        "        mean_loss = np.mean(loss_avg)\n",
        "        print(f'Loss: {mean_loss}')\n",
        "        scheduler.step(mean_loss)\n",
        "        loss_avg = []\n",
        "        model.eval()\n",
        "        predicted_text = evaluate(model, char_to_idx, idx_to_char)\n",
        "        print(predicted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Blc56qNsCWHd",
        "outputId": "2b1a6ed7-d179-4873-bea0-c9d4bd944f99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 1.7559614603673428\n",
            " давно бередность всем старик совершенно встречал всем старика ваше своей старик возможно стал стороны поднял подполь говорить совершенно поставила совершенно возьмите мог приведовал возможности смерти\n",
            "Loss: 1.5974770366191864\n",
            " дально просто поступиться совершенно подумал поставил случайно подумал всем слово только старик постороннейший дела поставляют половина самого слова свои половине последний столько того, всем какой-то\n",
            "Loss: 1.575976710820198\n",
            " ответил совершенно понимаете самом случае повернулся несколько собою столько принимает какой-то поставил столь собой подобного словах стола стала отвечал стороны своей воспоминания своим того, подобны\n",
            "Loss: 1.5650899735450745\n",
            " нашем странно просто странно совершенно весьма серьезно, видел всем воротился просто великое стороны привел наш всем покойно просто странно сказать, признаться собою моей последнего старик несколько с\n",
            "Loss: 1.5615136182785034\n",
            " нашей подле немедленно несколько положительно странно собой разговора недоумение пристально столько могло стал стал высшей стал слова приглашение помолковность представил себя, — прибавила своего подо\n",
            "Loss: 1.558972612285614\n",
            " он, слушал своего страшно же, пришло всем своего слова странно принял своего закустил столько возможности столько слова своей стало быть, должно совершенно слова нем настоящий старика слушал несколько\n",
            "Loss: 1.5601542387485505\n",
            " просто приговора высокоменный странного тотчас придавать привел отчасти нашего слова стану своих странно столько просил всем воспитание женщина привела постоянно смешно такое слова своим словами собст\n",
            "Loss: 1.5621586943864822\n",
            " пристально просто старика, слово совершенно стало быть, старик пристально положим, стало быть, все-таки просто сомнения столько подозревала зато стало быть, показались тебе половину собою того, соверш\n",
            "Loss: 1.5647698258638383\n",
            " подобные случае своей последнее воспоминания свет совершенно подобное ответили возможности того, который совершенно стал своей степени подошел последнего слова старику столь стал случайно просто своег\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "print(evaluate(\n",
        "    model,\n",
        "    char_to_idx,\n",
        "    idx_to_char,\n",
        "    start_text = 'А по темным улицам гуляет дождь',\n",
        "    temp=0.5,\n",
        "    prediction_len=100,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvnwK5IxCYfp",
        "outputId": "a8f8e3ae-50a0-4b33-824c-cb1fb3b11136"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "А по темным улицам гуляет дождь подозревала спрашивать вашим слабено мог могу разумеется, совершенно она, писали возможно подозрева\n"
          ]
        }
      ]
    }
  ]
}